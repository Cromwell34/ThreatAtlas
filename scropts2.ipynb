{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f188da",
   "metadata": {},
   "source": [
    "#### Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c97314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Stuff\n",
    "import feedparser\n",
    "import spacy\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f2fc1",
   "metadata": {},
   "source": [
    "#### Feed DB Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d434517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZzTHE\\AppData\\Local\\Temp\\ipykernel_229148\\1896297901.py:56: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def get_content(path, num_entries):\n",
    "    feed_db = pd.read_csv(path)\n",
    "    output_data = []\n",
    "\n",
    "    for index, row in feed_db.iterrows():\n",
    "        feed_name = row['feed_name']\n",
    "        feed_url = row['feed_url']\n",
    "        test = feedparser.parse(feed_url)\n",
    "\n",
    "        for i, entry in enumerate(test.entries):\n",
    "            if i >= num_entries:\n",
    "                break\n",
    "\n",
    "            title = get_field_safe(entry, 'title')\n",
    "            published = get_field_safe(entry, 'published')\n",
    "            link = get_field_safe(entry, 'link')\n",
    "            summary = get_field_safe(entry, 'summary')\n",
    "            id_ = get_field_safe(entry, 'id')\n",
    "            tags = get_field_safe(entry, 'tags')\n",
    "            author_names = get_field_safe(entry, 'author_names')\n",
    "\n",
    "            # Remove HTML tags from summary\n",
    "            if summary:\n",
    "                summary = remove_html_tags(summary)\n",
    "\n",
    "            output_data.append({\n",
    "                'Feed Name': feed_name,\n",
    "                'Title': title,\n",
    "                'Published': published,\n",
    "                'Link': link,\n",
    "                'Summary': summary,\n",
    "                'ID': id_,\n",
    "                'Tags': tags,\n",
    "                'Author Names': author_names\n",
    "            })\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    output_df.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "# Function to safely retrieve field or return None\n",
    "def get_field_safe(entry, field):\n",
    "    try:\n",
    "        if field == 'tags':\n",
    "            return [tag.term for tag in entry.tags]\n",
    "        elif field == 'author_names':\n",
    "            return [author.name for author in entry.authors]\n",
    "        else:\n",
    "            return getattr(entry, field)\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to remove HTML tags from a string\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path = r'C:\\Users\\ZzTHE\\Desktop\\local\\ThreatAtlas-Simons-10-06-2023-Branch\\test_feeds.csv'\n",
    "num_entries = 10  # Set the number of entries to parse\n",
    "get_content(path, num_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b516c",
   "metadata": {},
   "source": [
    "#### Event DB Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f60bbe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Trafficking in the Sahel: Muzzling the illicit arms trade\n",
      "Summary: Shoppers in Mali’s Gao, Timbuktu, and Ménaka regions can snap up AK-pattern assault rifles for $750 and cartridges for 70 cents apiece, from locally handcrafted pistols to smuggled French and Turkish machine guns, as a dizzying array of illegal weaponry dots market stalls across the Sahel, a 6,000-kilometre-wide belt in the middle of Africa. Read the full story, “Trafficking in the Sahel: Muzzling the illicit arms trade”, on globalissues.org →\n",
      "Tags: nan\n",
      "Countries: ['Gao', 'Mali', 'Timbuktu', 'Ménaka']\n",
      "Individuals: []\n",
      "Organizations: ['globalissues.org']\n",
      "Locations: ['Sahel', 'Africa']\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Set CSV path\n",
    "path = \"output.csv\"\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Select specific item data\n",
    "item_data = data.loc[10, ['Title', 'Summary', 'Tags']]\n",
    "\n",
    "# Extract countries\n",
    "def extract_countries(article):\n",
    "    doc = nlp(article)\n",
    "    countries = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            countries.append(ent.text)\n",
    "    \n",
    "    return list(set(countries))\n",
    "\n",
    "# Extract individuals\n",
    "def extract_individuals(article):\n",
    "    doc = nlp(article)\n",
    "    individuals = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            individuals.append(ent.text)\n",
    "    \n",
    "    return list(set(individuals))\n",
    "\n",
    "# Extract organizations\n",
    "def extract_organizations(article):\n",
    "    doc = nlp(article)\n",
    "    organizations = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            organizations.append(ent.text)\n",
    "    \n",
    "    return list(set(organizations))\n",
    "\n",
    "# Extract locations\n",
    "def extract_locations(article):\n",
    "    doc = nlp(article)\n",
    "    locations = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"LOC\":\n",
    "            locations.append(ent.text)\n",
    "    \n",
    "    return list(set(locations))\n",
    "\n",
    "# Extract individual elements from the Series\n",
    "title = str(item_data['Title'])\n",
    "summary = str(item_data['Summary'])\n",
    "tags = str(item_data['Tags'])\n",
    "\n",
    "# Print the item data and extracted information\n",
    "print(\"Title:\", title)\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Tags:\", tags)\n",
    "\n",
    "# Combine the text from title, summary, and tags\n",
    "combined_text = title + \" \" + summary + \" \" + tags\n",
    "\n",
    "# Extract and print the entities\n",
    "print(\"Countries:\", extract_countries(combined_text))\n",
    "print(\"Individuals:\", extract_individuals(combined_text))\n",
    "print(\"Organizations:\", extract_organizations(combined_text))\n",
    "print(\"Locations:\", extract_locations(combined_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35209ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
