{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major script stash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, timedelta, date\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from dateparser import parse\n",
    "import feedparser\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles_from_rss(feed_url, source):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "\n",
    "        articles.append(entry)\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = 'test_feeds.csv'\n",
    "def get_content(path):\n",
    "    feed_db = pd.read_csv(path)\n",
    "\n",
    "    # zip relevant cols \n",
    "    reqs = list(zip(feed_db['feed_name'].tolist(),feed_db['feed_url'].tolist()))\n",
    "    # fetch articles\n",
    "    \n",
    "    article_dump = []\n",
    "    for req in reqs:\n",
    "        articles = fetch_articles_from_rss(req[1],req[0])\n",
    "        article_dump.append((req[0],articles))\n",
    "        # print(\"  \")\n",
    "        # print(\"Fetched successfully \"+\"feed: \"+req[0])\n",
    "        # print([article['title'] for article in articles])\n",
    "\n",
    "\n",
    "    return dict(article_dump)\n",
    "\n",
    "\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "        \n",
    "def output_to_csv(sources, data):\n",
    "    dump = []\n",
    "    for source in sources: \n",
    "        for article_ in data[source]:\n",
    "            article = dict(article_)\n",
    "            dump.append(article)\n",
    "\n",
    "    test_db = pd.DataFrame(dump)\n",
    "    test_db.to_csv('test_output.csv')\n",
    "    return test_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_content(path)\n",
    "sources = list(test.keys())\n",
    "test_db = output_to_csv(sources, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def extract_countries(article):\n",
    "    doc = nlp(article)\n",
    "    countries = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"NORP\" or ent.label_ == \"LOC\":\n",
    "            countries.append(ent.text)\n",
    "    \n",
    "    return list(set(countries))\n",
    "\n",
    "def extract_individuals(article):\n",
    "    doc = nlp(article)\n",
    "    individuals = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            individuals.append(ent.text)\n",
    "    \n",
    "    return list(set(individuals))\n",
    "\n",
    "def extract_organizations(article):\n",
    "    doc = nlp(article)\n",
    "    organizations = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            organizations.append(ent.text)\n",
    "    \n",
    "    return list(set(organizations))\n",
    "\n",
    "def key_phrases(article):\n",
    "    phrases = []\n",
    "    doc = nlp(article)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrases.append(chunk)\n",
    "        \n",
    "    return phrases\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_event(article):\n",
    "    countries = extract_countries(article)\n",
    "    individuals = extract_individuals(article)\n",
    "    organizations = extract_organizations(article)\n",
    "    phrases = key_phrases(article)\n",
    "    url = None\n",
    "\n",
    "\n",
    "    current_event = {\n",
    "        \"countries_involved\": countries,\n",
    "        \"associated_individuals\": individuals,\n",
    "        \"associated_organizations\": organizations,\n",
    "        \"key_phrases\": phrases,\n",
    "    }\n",
    "    \n",
    "    return current_event\n",
    "# Example usage\n",
    "article = \"From the Field: UN human rights officers on the frontline in Somalia Fleeing armed conflict is frightening, forcing people to escape violent clashes and leave behind their homes, schools, and daily lives in a desperate search for safety. Read the full story, â€œFrom the Field: UN human rights officers on the frontline in Somalia\"\n",
    "event = extract_event(article)\n",
    "# print(\"Description:\", event.get(\"description\"))\n",
    "print(\"Countries involved:\", event.get(\"countries_involved\"))\n",
    "print(\"Associated individuals:\", event.get(\"associated_individuals\"))\n",
    "print(\"Associated organizations:\", event.get(\"associated_organizations\"))\n",
    "print(\"Key phrases:\", event.get(\"key_phrases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract text from news articles \n",
    "articles = []\n",
    "headers = ['Countries involved', 'Date', 'Associated Individuals',\n",
    "           'Associated Organizations', 'Countries Involved', 'Location', 'url' ]\n",
    "\n",
    "def eventlist(articles):\n",
    "    events = pd.dataframe(names=headers)\n",
    "    for article in articles:\n",
    "        extract_event(article)\n",
    "\n",
    "    return events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDELT Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = int(input(\"1) get the latest file 2) get a date range\"))\n",
    "header = \"GLOBALEVENTID SQLDATE MonthYear Year FractionDate Actor1Code Actor1Name Actor1CountryCode Actor1KnownGroupCode Actor1EthnicCode Actor1Religion1Cod Actor1Religion2Code Actor1Type1Code Actor1Type2Code Actor1Type3Code Actor2Code Actor2Name Actor2CountryCode Actor2KnownGroupCode Actor2EthnicCode Actor2Religion1Code Actor2Religion2Code Actor2Type1Code Actor2Type2Code Actor2Type3Code IsRootEvent EventCode EventBaseCode EventRootCode QuadClass GoldsteinScale NumMentions NumSources NumArticles AvgTone Actor1Geo_Type Actor1Geo_FullName Actor1Geo_CountryCode Actor1Geo_ADM1Code Actor1Geo_Lat Actor1Geo_Long Actor1Geo_FeatureID Actor2Geo_Type Actor2Geo_FullName Actor2Geo_CountryCode Actor2Geo_ADM1Code Actor2Geo_Lat Actor2Geo_Long Actor2Geo_FeatureID ActionGeo_Type ActionGeo_FullName ActionGeo_CountryCode ActionGeo_ADM1Code ActionGeo_Lat ActionGeo_Long ActionGeo_FeatureID DATEADDED SOURCEURL\".split(\" \")\n",
    "\n",
    "if selection == 2:\n",
    "    print(\"Downloading GDELT Events 1.0 files\")\n",
    "\n",
    "    start_date = datetime(2022, 11, 11)\n",
    "    end_date = datetime(2023, 2, 7)\n",
    "\n",
    "    def date_range(start, end):\n",
    "        delta = end - start  # as timedelta\n",
    "        days = [start + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "        return days\n",
    "\n",
    "    days = [str(day.strftime(\"%Y%m%d\")) for day in date_range(start_date, end_date)]\n",
    "\n",
    "    for day in days:\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://data.gdeltproject.org/events/\" + day + \".export.CSV.zip\",\n",
    "            \"gdelt_extraction/dump/\" + \"GEvents1\" + day + \".zip\",\n",
    "        )\n",
    "\n",
    "    print(\"Unzipping files\")\n",
    "    test_dir = os.listdir(\"gdelt_extraction/dump\")\n",
    "    for n in test_dir:\n",
    "        with ZipFile(\"gdelt_extraction/dump/\" + n, \"r\") as zipObj:\n",
    "            zipObj.extractall(\"gdelt_extraction/dump\")\n",
    "\n",
    "    print(\"Adding headers\")\n",
    "    # process and save files\n",
    "    test_dir = os.listdir(\"gdelt_extraction/dump\")\n",
    "    for p in test_dir:\n",
    "        if \".CSV\" in p:\n",
    "            data_test = pd.read_csv(\n",
    "                \"gdelt_extraction/dump/\" + p, delimiter=\"\\t\", names=header\n",
    "            )\n",
    "            data_test.to_csv(\"gdelt_extraction/results/\" + p + \"_processed\" + \".csv\")\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # deletes the crap from the gdelt_extraction folder\n",
    "    gdelt_extraction_dir = glob.glob(\"gdelt_extraction/dump/*\")\n",
    "    for f in gdelt_extraction_dir:\n",
    "        os.remove(f)\n",
    "\n",
    "    with open(\"gdelt_extraction/cameo_dict.csv\", mode=\"r\", encoding=\"utf-8\") as inp:\n",
    "        reader = csv.reader(inp)\n",
    "        cameo_dict = {rows[0]: rows[2] for rows in reader}\n",
    "\n",
    "    test_dir = os.listdir(\"gdelt_extraction/results\")\n",
    "    for f in test_dir:\n",
    "        convert_dict = {\"EventCode\": str}\n",
    "\n",
    "        df2 = pd.read_csv(\"gdelt_extraction/results/\" + f)\n",
    "        df2 = df2.astype(convert_dict)\n",
    "        df2.replace({\"EventCode\": cameo_dict}, inplace=True)\n",
    "\n",
    "        print(df2[\"EventCode\"])\n",
    "        df2.to_csv(\"gdelt_extraction/results_cleaned/\" + f + \"_coded\" + \".csv\")\n",
    "\n",
    "    gdelt_extraction_dir = glob.glob(\"gdelt_extraction/results_cleaned/*\")\n",
    "    for f in gdelt_extraction_dir:\n",
    "        os.remove(f)\n",
    "\n",
    "if selection == 1:\n",
    "    print(\"Downloading the latest file\")\n",
    "\n",
    "    yesterday = (date.today() - timedelta(1)).strftime(\"%Y%m%d\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://data.gdeltproject.org/events/\" + yesterday + \".export.CSV.zip\",\n",
    "        \"gdelt_extraction/dump/\" + \"GEvents\" + yesterday + \".zip\",\n",
    "    )\n",
    "\n",
    "    print(\"Unzipping files\")\n",
    "    with ZipFile(\"gdelt_extraction/dump/\" + \"GEvents\" + yesterday + \".zip\") as zipObj:\n",
    "        zipObj.extractall(\"gdelt_extraction/dump\")\n",
    "\n",
    "    print(\"Adding headers\")\n",
    "    data_test = pd.read_csv(\n",
    "        \"gdelt_extraction/dump/\" + \"GEvents\" + yesterday + \".zip\",\n",
    "        delimiter=\"\\t\",\n",
    "        names=header,\n",
    "    )\n",
    "    data_test.to_csv(\n",
    "        \"gdelt_extraction/results/\" + \"GEvents\" + yesterday + \"_processed\" + \".csv\"\n",
    "    )\n",
    "\n",
    "print(\"Process completed!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "import json\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "def flat_ents1(ents):\n",
    "    flat_ents = []\n",
    "    for ent in ents:\n",
    "        flat_ents = flat_ents + ent[1]\n",
    "\n",
    "    return flat_ents\n",
    "\n",
    "def top_n_bar_chart(data, n,var_name):\n",
    "    x = [k[0] for k in data[0:n+1]]\n",
    "    y = [k[1] for k in data[0:n+1]]\n",
    "    data = [go.Bar(x=x, y=y)]\n",
    "    layout = go.Layout(title=f\"Top {n} \"+var_name,template=\"plotly_dark\" )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set = pd.read_json('output.json')\n",
    "test_set.to_csv('piss.csv')\n",
    "\n",
    "places = list(zip(list(range(len(test_set[\"Countries\"].tolist()))),test_set[\"Countries\"].tolist()))\n",
    "# orgs = list(zip(list(range(len(test_set[\"Associated Organizations\"].tolist()))),test_set[\"Associated Organizations\"].tolist()))\n",
    "titles = test_set['headline'].tolist()\n",
    "# summmaries = test_set['Summary'].tolist()\n",
    "\n",
    "all_places = flat_ents1(places)\n",
    "places_ = {i:all_places.count(i) for i in all_places}\n",
    "places_dict = sorted(places_.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "all_orgs = flat_ents1(orgs)\n",
    "orgs_ = {i:all_orgs.count(i) for i in all_orgs}\n",
    "orgs_dict = sorted(orgs_.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "def new_count(field,frame):\n",
    "    indexed = list(zip(list(range(len(frame[field].tolist()))),frame[field].tolist()))\n",
    "    all_ = flat_ents1(indexed)\n",
    "    counted_ = {i:all_.count(i) for i in all_}\n",
    "    counted = sorted(counted_.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    return counted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 'Feed Name', 'Title', 'Date', 'Link', 'Summary', 'ID', 'Tags','Author Names', 'Countries', 'Associated Individuals','Associated Organizations', 'Key Phrases'\n",
    "# top_n_bar_chart(orgs_dict, 25,'organizations')\n",
    "# top_n_bar_chart(new_count('Associated Individuals',test_db), 25,'individuals')\n",
    "\n",
    "top_ent = [k[0] for k in places_dict[0:26]]\n",
    "s = test_set['Countries'].explode()=='Ukraine'\n",
    "arts = test_set.loc[s[s].index]['Title'].tolist()\n",
    "\n",
    "# for art in arts: \n",
    "#     display(Markdown(art))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
